We need to take node01 out for maintenance. Empty the node of all applications and mark it unschedulable.
kubectl drain node01 --ignore-daemonsets
This command:
Evicts all pods from the node (except DaemonSet pods).
Ensures that no new pods are scheduled on the node

kubectl cordon node01 --->no new pods assigned to node01 until it set to uncordon state

kubectl uncordon node01  --->The maintenance tasks have been completed. Configure the node node01 to be schedulable again

Why are the pods placed on the controlplane node? ----> Controlplane does not have taints

#################################################################################################

Upgrading a cluster involves 2 major steps
There are different strategies that are available to upgrade the worker nodes
One is to upgrade all at once. But then your pods will be down and users will not be able to access the applications.

Second one is to upgrade one node at a time.

Third one would be to add new nodes to the cluster


kubeadm - Upgrade master node
kubeadm has an upgrade command that helps in upgrading clusters.

kubeadm upgrade plan 

Upgrade kubeadm from v1.11 to v1.12

$ apt-get upgrade -y kubeadm=1.12.0-00
Upgrade the cluster

$ kubeadm upgrade apply v1.12.0
If you run the 'kubectl get nodes' command, you will see the older version. This is because in the output of the command it is showing the versions of kubelets on each of these nodes registered with the API Server and not the version of API Server itself

$ kubectl get nodes

Upgrade 'kubelet' on the master node

$ apt-get upgrade kubelet=1.12.0-00
Restart the kubelet

$ systemctl restart kubelet
Run 'kubectl get nodes' to verify

$ kubectl get nodes

kubeadm - Upgrade worker nodes
From master node, run 'kubectl drain' command to move the workloads to other nodes

$ kubectl drain node-1
Upgrade kubeadm and kubelet packages

$ apt-get upgrade -y kubeadm=1.12.0-00
$ apt-get upgrade -y kubelet=1.12.0-00
Update the node configuration for the new kubelet version

$ kubeadm upgrade node config --kubelet-version v1.12.0
Restart the kubelet service

$ systemctl restart kubelet
Mark the node back to schedulable
$ kubectl uncordon node-1

Upgrade all worker nodes in the same way


#################################################################################################

steps to Update an EKS Cluster
1. Check the Current EKS Cluster Version
Run the following command to check the current Kubernetes version of your cluster:

aws eks describe-cluster --name <cluster-name> --query "cluster.version" --output text

2. Check Available Updates
List available Kubernetes versions for your region:

aws eks describe-addon-versions --query "addons[].kubernetesVersions" --output text

3. Update the Control Plane
To update the EKS control plane to the latest version:

aws eks update-cluster-version --name <cluster-name> --kubernetes-version <target-version>

4. Update Node Groups
a. Update Managed Node Groups
For managed node groups, update the node group's Kubernetes version:

aws eks update-nodegroup-version --cluster-name <cluster-name> --nodegroup-name <node-group-name> --kubernetes-version <target-version>

kubectl drain <node-name> --ignore-daemonsets --delete-emptydir-data

5. Update Add-ons
After updating the cluster and nodes, update the EKS add-ons (e.g., CoreDNS, kube-proxy).

Check Add-ons Versions
bash
Copy code
aws eks describe-cluster --name <cluster-name> --query "cluster.addons" --output json
Update Add-ons
For example, to update CoreDNS:

aws eks update-addon --cluster-name <cluster-name> --addon-name coredns --addon-version <target-version>
Monitor the add-on update:

aws eks describe-addon --cluster-name <cluster-name> --addon-name coredns --query "addon.status" --output text

6. Verify the Update
After all updates:
Confirm the Kubernetes version:
kubectl version --short
Check node readiness:

kubectl get nodes
Validate add-ons:

kubectl get pods -n kube-system

#####################################################################################################################

Backup and Restore Methods:

2. Application-Level Backup and Restore
Backing Up Resources
Use kubectl to export YAML definitions of all resources:


kubectl get all --all-namespaces -o yaml > cluster-backup.yaml
Restoring Resources
Apply the YAML file to restore resources:

kubectl apply -f cluster-backup.yaml
Note: This does not backup Persistent Volumes (PVs) or data stored within pods.

You can take a snapshot of the etcd database by using etcdctl utility snapshot save command.

ETCDCTL_API=3 etcdctl snapshot save snapshot.db
ETCDCTL_API=3 etcdctl snapshot status snapshot.db

Restore - ETCD
To restore etcd from the backup at later in time. First stop kube-apiserver service

 service kube-apiserver stop
Run the etcdctl snapshot restore command

Update the etcd service

Reload system configs

systemctl daemon-reload
Restart etcd

service etcd restart

Start the kube-apiserver

 service kube-apiserver start

With all etcdctl commands specify the cert,key,cacert and endpoint for authentication.
ETCDCTL_API=3 etcdctl \
  snapshot save /tmp/snapshot.db \
  --endpoints=https://[127.0.0.1]:2379 \
  --cacert=/etc/kubernetes/pki/etcd/ca.crt \
  --cert=/etc/kubernetes/pki/etcd/etcd-server.crt \
  --key=/etc/kubernetes/pki/etcd/etcd-server.key
Other way:
Using Tools for Cluster Backup
Several tools automate cluster and application backups:

Velero
#############################################################################################################







